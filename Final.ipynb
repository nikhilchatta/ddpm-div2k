{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abaaef6-8a95-46d5-9166-1d4fa0087b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ddpm-div2k'...\n",
      "remote: Enumerating objects: 2610, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 2610 (delta 5), reused 5 (delta 1), pack-reused 2595 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2610/2610), 169.92 MiB | 57.46 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n",
      "Updating files: 100% (2612/2612), done.\n",
      "Filtering content: 100% (3/3), 381.08 MiB | 97.87 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nikhilchatta/DDPM-CIFAR10-Clean.git ddpm-div2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a884d1-14cc-44d2-9ef1-cbff5fe66d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/ddpm-div2k\n"
     ]
    }
   ],
   "source": [
    "cd ddpm-div2k/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c0fbff-fdf2-4d0f-9b04-244792826f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[0m\n",
      "├── \u001b[01;34mCheckpoints\u001b[0m\n",
      "│   └── final_model.pt\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "│   ├── \u001b[01;34mDIV2K_train_HR\u001b[0m\n",
      "│   └── \u001b[01;31mDIV2K_train_HR.zip\u001b[0m\n",
      "├── \u001b[01;34mDenoisingDiffusionProbabilityModel-ddpm-\u001b[0m\n",
      "├── \u001b[01;34mDiffusion\u001b[0m\n",
      "│   ├── Diffusion.py\n",
      "│   ├── div2k_dataloader.py\n",
      "│   ├── __init__.py\n",
      "│   ├── Model.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   ├── Train.py\n",
      "│   └── Unet.py\n",
      "├── \u001b[01;34mDiffusionFreeGuidence\u001b[0m\n",
      "│   ├── DiffusionCondition.py\n",
      "│   ├── __init__.py\n",
      "│   ├── ModelCondition.py\n",
      "│   └── TrainCondition.py\n",
      "├── MainCondition.py\n",
      "├── Main.py\n",
      "├── Metrics.ipynb\n",
      "├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   └── Scheduler.cpython-39.pyc\n",
      "├── README.md\n",
      "└── Scheduler.py\n",
      "\n",
      "8 directories, 18 files\n"
     ]
    }
   ],
   "source": [
    "!tree . -L 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc0f13cc-3afe-4866-8026-58eeea42079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training on device: cpu\n",
      "/users/PFS0270/nikhilchatta/ddpm-div2k/Diffusion/div2k_dataloader.py:22: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  image = (torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n",
      "Epoch [1/5], Loss: 2.7971\n",
      "Epoch [2/5], Loss: 0.6886\n",
      "Epoch [3/5], Loss: 0.3190\n",
      "Epoch [4/5], Loss: 0.1982\n",
      "Epoch [5/5], Loss: 0.1340\n",
      "✅ Model saved to: ./Checkpoints/final_model.pt\n"
     ]
    }
   ],
   "source": [
    "!python Main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "433c67a7-9c05-42f9-aaa4-da2198274e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Your same UNet used in training ===\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, T, ch=128, ch_mult=[1, 2, 3, 4], attn=[2], num_res_blocks=2, dropout=0.1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.T = T\n",
    "        self.ch = ch\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(T, ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ch, ch * 2)\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ch, ch * 2, kernel_size=3, padding=1)\n",
    "        self.deconv = nn.ConvTranspose2d(ch * 2, ch, kernel_size=3, padding=1)\n",
    "        self.out_conv = nn.Conv2d(ch, 3, kernel_size=3, padding=1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self._timestep_embedding(t, self.T)\n",
    "        t_embed = self.time_embed(t_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.act(self.conv1(x) + t_embed[:, :self.ch])\n",
    "        x = self.act(self.conv2(x) + t_embed)\n",
    "        x = self.act(self.deconv(x) + t_embed[:, :self.ch])\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "\n",
    "    def _timestep_embedding(self, timesteps, dim):\n",
    "        half = dim // 2\n",
    "        emb = torch.exp(torch.arange(half, dtype=torch.float32, device=timesteps.device) * -torch.log(torch.tensor(10000.0)) / (half - 1))\n",
    "        emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "        return emb\n",
    "\n",
    "# === Your same GaussianDiffusionSampler ===\n",
    "from Diffusion.Diffusion import GaussianDiffusionSampler\n",
    "\n",
    "def generate_one_batch(model_path, save_dir, batch_size=64, image_size=128, device=\"cpu\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model = UNet(\n",
    "        T=1000,\n",
    "        ch=128,\n",
    "        ch_mult=[1, 2, 3, 4],\n",
    "        attn=[2],\n",
    "        num_res_blocks=2,\n",
    "        dropout=0.15\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    diffusion = GaussianDiffusionSampler(model, beta_1=1e-4, beta_T=0.02, T=1000).to(device)\n",
    "\n",
    "    x_T = torch.randn((batch_size, 3, image_size, image_size)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sampled_imgs = diffusion(x_T)\n",
    "        sampled_imgs = (sampled_imgs.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        save_image(sampled_imgs[i], os.path.join(save_dir, f\"sample_{i+1}.png\"))\n",
    "\n",
    "    print(f\"✅ Generated {batch_size} images in: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "358165c3-4e5c-4ec7-a509-3ecca4ed6033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurmtmp.760800/ipykernel_3742497/1297012076.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "998\n",
      "997\n",
      "996\n",
      "995\n",
      "994\n",
      "993\n",
      "992\n",
      "991\n",
      "990\n",
      "989\n",
      "988\n",
      "987\n",
      "986\n",
      "985\n",
      "984\n",
      "983\n",
      "982\n",
      "981\n",
      "980\n",
      "979\n",
      "978\n",
      "977\n",
      "976\n",
      "975\n",
      "974\n",
      "973\n",
      "972\n",
      "971\n",
      "970\n",
      "969\n",
      "968\n",
      "967\n",
      "966\n",
      "965\n",
      "964\n",
      "963\n",
      "962\n",
      "961\n",
      "960\n",
      "959\n",
      "958\n",
      "957\n",
      "956\n",
      "955\n",
      "954\n",
      "953\n",
      "952\n",
      "951\n",
      "950\n",
      "949\n",
      "948\n",
      "947\n",
      "946\n",
      "945\n",
      "944\n",
      "943\n",
      "942\n",
      "941\n",
      "940\n",
      "939\n",
      "938\n",
      "937\n",
      "936\n",
      "935\n",
      "934\n",
      "933\n",
      "932\n",
      "931\n",
      "930\n",
      "929\n",
      "928\n",
      "927\n",
      "926\n",
      "925\n",
      "924\n",
      "923\n",
      "922\n",
      "921\n",
      "920\n",
      "919\n",
      "918\n",
      "917\n",
      "916\n",
      "915\n",
      "914\n",
      "913\n",
      "912\n",
      "911\n",
      "910\n",
      "909\n",
      "908\n",
      "907\n",
      "906\n",
      "905\n",
      "904\n",
      "903\n",
      "902\n",
      "901\n",
      "900\n",
      "899\n",
      "898\n",
      "897\n",
      "896\n",
      "895\n",
      "894\n",
      "893\n",
      "892\n",
      "891\n",
      "890\n",
      "889\n",
      "888\n",
      "887\n",
      "886\n",
      "885\n",
      "884\n",
      "883\n",
      "882\n",
      "881\n",
      "880\n",
      "879\n",
      "878\n",
      "877\n",
      "876\n",
      "875\n",
      "874\n",
      "873\n",
      "872\n",
      "871\n",
      "870\n",
      "869\n",
      "868\n",
      "867\n",
      "866\n",
      "865\n",
      "864\n",
      "863\n",
      "862\n",
      "861\n",
      "860\n",
      "859\n",
      "858\n",
      "857\n",
      "856\n",
      "855\n",
      "854\n",
      "853\n",
      "852\n",
      "851\n",
      "850\n",
      "849\n",
      "848\n",
      "847\n",
      "846\n",
      "845\n",
      "844\n",
      "843\n",
      "842\n",
      "841\n",
      "840\n",
      "839\n",
      "838\n",
      "837\n",
      "836\n",
      "835\n",
      "834\n",
      "833\n",
      "832\n",
      "831\n",
      "830\n",
      "829\n",
      "828\n",
      "827\n",
      "826\n",
      "825\n",
      "824\n",
      "823\n",
      "822\n",
      "821\n",
      "820\n",
      "819\n",
      "818\n",
      "817\n",
      "816\n",
      "815\n",
      "814\n",
      "813\n",
      "812\n",
      "811\n",
      "810\n",
      "809\n",
      "808\n",
      "807\n",
      "806\n",
      "805\n",
      "804\n",
      "803\n",
      "802\n",
      "801\n",
      "800\n",
      "799\n",
      "798\n",
      "797\n",
      "796\n",
      "795\n",
      "794\n",
      "793\n",
      "792\n",
      "791\n",
      "790\n",
      "789\n",
      "788\n",
      "787\n",
      "786\n",
      "785\n",
      "784\n",
      "783\n",
      "782\n",
      "781\n",
      "780\n",
      "779\n",
      "778\n",
      "777\n",
      "776\n",
      "775\n",
      "774\n",
      "773\n",
      "772\n",
      "771\n",
      "770\n",
      "769\n",
      "768\n",
      "767\n",
      "766\n",
      "765\n",
      "764\n",
      "763\n",
      "762\n",
      "761\n",
      "760\n",
      "759\n",
      "758\n",
      "757\n",
      "756\n",
      "755\n",
      "754\n",
      "753\n",
      "752\n",
      "751\n",
      "750\n",
      "749\n",
      "748\n",
      "747\n",
      "746\n",
      "745\n",
      "744\n",
      "743\n",
      "742\n",
      "741\n",
      "740\n",
      "739\n",
      "738\n",
      "737\n",
      "736\n",
      "735\n",
      "734\n",
      "733\n",
      "732\n",
      "731\n",
      "730\n",
      "729\n",
      "728\n",
      "727\n",
      "726\n",
      "725\n",
      "724\n",
      "723\n",
      "722\n",
      "721\n",
      "720\n",
      "719\n",
      "718\n",
      "717\n",
      "716\n",
      "715\n",
      "714\n",
      "713\n",
      "712\n",
      "711\n",
      "710\n",
      "709\n",
      "708\n",
      "707\n",
      "706\n",
      "705\n",
      "704\n",
      "703\n",
      "702\n",
      "701\n",
      "700\n",
      "699\n",
      "698\n",
      "697\n",
      "696\n",
      "695\n",
      "694\n",
      "693\n",
      "692\n",
      "691\n",
      "690\n",
      "689\n",
      "688\n",
      "687\n",
      "686\n",
      "685\n",
      "684\n",
      "683\n",
      "682\n",
      "681\n",
      "680\n",
      "679\n",
      "678\n",
      "677\n",
      "676\n",
      "675\n",
      "674\n",
      "673\n",
      "672\n",
      "671\n",
      "670\n",
      "669\n",
      "668\n",
      "667\n",
      "666\n",
      "665\n",
      "664\n",
      "663\n",
      "662\n",
      "661\n",
      "660\n",
      "659\n",
      "658\n",
      "657\n",
      "656\n",
      "655\n",
      "654\n",
      "653\n",
      "652\n",
      "651\n",
      "650\n",
      "649\n",
      "648\n",
      "647\n",
      "646\n",
      "645\n",
      "644\n",
      "643\n",
      "642\n",
      "641\n",
      "640\n",
      "639\n",
      "638\n",
      "637\n",
      "636\n",
      "635\n",
      "634\n",
      "633\n",
      "632\n",
      "631\n",
      "630\n",
      "629\n",
      "628\n",
      "627\n",
      "626\n",
      "625\n",
      "624\n",
      "623\n",
      "622\n",
      "621\n",
      "620\n",
      "619\n",
      "618\n",
      "617\n",
      "616\n",
      "615\n",
      "614\n",
      "613\n",
      "612\n",
      "611\n",
      "610\n",
      "609\n",
      "608\n",
      "607\n",
      "606\n",
      "605\n",
      "604\n",
      "603\n",
      "602\n",
      "601\n",
      "600\n",
      "599\n",
      "598\n",
      "597\n",
      "596\n",
      "595\n",
      "594\n",
      "593\n",
      "592\n",
      "591\n",
      "590\n",
      "589\n",
      "588\n",
      "587\n",
      "586\n",
      "585\n",
      "584\n",
      "583\n",
      "582\n",
      "581\n",
      "580\n",
      "579\n",
      "578\n",
      "577\n",
      "576\n",
      "575\n",
      "574\n",
      "573\n",
      "572\n",
      "571\n",
      "570\n",
      "569\n",
      "568\n",
      "567\n",
      "566\n",
      "565\n",
      "564\n",
      "563\n",
      "562\n",
      "561\n",
      "560\n",
      "559\n",
      "558\n",
      "557\n",
      "556\n",
      "555\n",
      "554\n",
      "553\n",
      "552\n",
      "551\n",
      "550\n",
      "549\n",
      "548\n",
      "547\n",
      "546\n",
      "545\n",
      "544\n",
      "543\n",
      "542\n",
      "541\n",
      "540\n",
      "539\n",
      "538\n",
      "537\n",
      "536\n",
      "535\n",
      "534\n",
      "533\n",
      "532\n",
      "531\n",
      "530\n",
      "529\n",
      "528\n",
      "527\n",
      "526\n",
      "525\n",
      "524\n",
      "523\n",
      "522\n",
      "521\n",
      "520\n",
      "519\n",
      "518\n",
      "517\n",
      "516\n",
      "515\n",
      "514\n",
      "513\n",
      "512\n",
      "511\n",
      "510\n",
      "509\n",
      "508\n",
      "507\n",
      "506\n",
      "505\n",
      "504\n",
      "503\n",
      "502\n",
      "501\n",
      "500\n",
      "499\n",
      "498\n",
      "497\n",
      "496\n",
      "495\n",
      "494\n",
      "493\n",
      "492\n",
      "491\n",
      "490\n",
      "489\n",
      "488\n",
      "487\n",
      "486\n",
      "485\n",
      "484\n",
      "483\n",
      "482\n",
      "481\n",
      "480\n",
      "479\n",
      "478\n",
      "477\n",
      "476\n",
      "475\n",
      "474\n",
      "473\n",
      "472\n",
      "471\n",
      "470\n",
      "469\n",
      "468\n",
      "467\n",
      "466\n",
      "465\n",
      "464\n",
      "463\n",
      "462\n",
      "461\n",
      "460\n",
      "459\n",
      "458\n",
      "457\n",
      "456\n",
      "455\n",
      "454\n",
      "453\n",
      "452\n",
      "451\n",
      "450\n",
      "449\n",
      "448\n",
      "447\n",
      "446\n",
      "445\n",
      "444\n",
      "443\n",
      "442\n",
      "441\n",
      "440\n",
      "439\n",
      "438\n",
      "437\n",
      "436\n",
      "435\n",
      "434\n",
      "433\n",
      "432\n",
      "431\n",
      "430\n",
      "429\n",
      "428\n",
      "427\n",
      "426\n",
      "425\n",
      "424\n",
      "423\n",
      "422\n",
      "421\n",
      "420\n",
      "419\n",
      "418\n",
      "417\n",
      "416\n",
      "415\n",
      "414\n",
      "413\n",
      "412\n",
      "411\n",
      "410\n",
      "409\n",
      "408\n",
      "407\n",
      "406\n",
      "405\n",
      "404\n",
      "403\n",
      "402\n",
      "401\n",
      "400\n",
      "399\n",
      "398\n",
      "397\n",
      "396\n",
      "395\n",
      "394\n",
      "393\n",
      "392\n",
      "391\n",
      "390\n",
      "389\n",
      "388\n",
      "387\n",
      "386\n",
      "385\n",
      "384\n",
      "383\n",
      "382\n",
      "381\n",
      "380\n",
      "379\n",
      "378\n",
      "377\n",
      "376\n",
      "375\n",
      "374\n",
      "373\n",
      "372\n",
      "371\n",
      "370\n",
      "369\n",
      "368\n",
      "367\n",
      "366\n",
      "365\n",
      "364\n",
      "363\n",
      "362\n",
      "361\n",
      "360\n",
      "359\n",
      "358\n",
      "357\n",
      "356\n",
      "355\n",
      "354\n",
      "353\n",
      "352\n",
      "351\n",
      "350\n",
      "349\n",
      "348\n",
      "347\n",
      "346\n",
      "345\n",
      "344\n",
      "343\n",
      "342\n",
      "341\n",
      "340\n",
      "339\n",
      "338\n",
      "337\n",
      "336\n",
      "335\n",
      "334\n",
      "333\n",
      "332\n",
      "331\n",
      "330\n",
      "329\n",
      "328\n",
      "327\n",
      "326\n",
      "325\n",
      "324\n",
      "323\n",
      "322\n",
      "321\n",
      "320\n",
      "319\n",
      "318\n",
      "317\n",
      "316\n",
      "315\n",
      "314\n",
      "313\n",
      "312\n",
      "311\n",
      "310\n",
      "309\n",
      "308\n",
      "307\n",
      "306\n",
      "305\n",
      "304\n",
      "303\n",
      "302\n",
      "301\n",
      "300\n",
      "299\n",
      "298\n",
      "297\n",
      "296\n",
      "295\n",
      "294\n",
      "293\n",
      "292\n",
      "291\n",
      "290\n",
      "289\n",
      "288\n",
      "287\n",
      "286\n",
      "285\n",
      "284\n",
      "283\n",
      "282\n",
      "281\n",
      "280\n",
      "279\n",
      "278\n",
      "277\n",
      "276\n",
      "275\n",
      "274\n",
      "273\n",
      "272\n",
      "271\n",
      "270\n",
      "269\n",
      "268\n",
      "267\n",
      "266\n",
      "265\n",
      "264\n",
      "263\n",
      "262\n",
      "261\n",
      "260\n",
      "259\n",
      "258\n",
      "257\n",
      "256\n",
      "255\n",
      "254\n",
      "253\n",
      "252\n",
      "251\n",
      "250\n",
      "249\n",
      "248\n",
      "247\n",
      "246\n",
      "245\n",
      "244\n",
      "243\n",
      "242\n",
      "241\n",
      "240\n",
      "239\n",
      "238\n",
      "237\n",
      "236\n",
      "235\n",
      "234\n",
      "233\n",
      "232\n",
      "231\n",
      "230\n",
      "229\n",
      "228\n",
      "227\n",
      "226\n",
      "225\n",
      "224\n",
      "223\n",
      "222\n",
      "221\n",
      "220\n",
      "219\n",
      "218\n",
      "217\n",
      "216\n",
      "215\n",
      "214\n",
      "213\n",
      "212\n",
      "211\n",
      "210\n",
      "209\n",
      "208\n",
      "207\n",
      "206\n",
      "205\n",
      "204\n",
      "203\n",
      "202\n",
      "201\n",
      "200\n",
      "199\n",
      "198\n",
      "197\n",
      "196\n",
      "195\n",
      "194\n",
      "193\n",
      "192\n",
      "191\n",
      "190\n",
      "189\n",
      "188\n",
      "187\n",
      "186\n",
      "185\n",
      "184\n",
      "183\n",
      "182\n",
      "181\n",
      "180\n",
      "179\n",
      "178\n",
      "177\n",
      "176\n",
      "175\n",
      "174\n",
      "173\n",
      "172\n",
      "171\n",
      "170\n",
      "169\n",
      "168\n",
      "167\n",
      "166\n",
      "165\n",
      "164\n",
      "163\n",
      "162\n",
      "161\n",
      "160\n",
      "159\n",
      "158\n",
      "157\n",
      "156\n",
      "155\n",
      "154\n",
      "153\n",
      "152\n",
      "151\n",
      "150\n",
      "149\n",
      "148\n",
      "147\n",
      "146\n",
      "145\n",
      "144\n",
      "143\n",
      "142\n",
      "141\n",
      "140\n",
      "139\n",
      "138\n",
      "137\n",
      "136\n",
      "135\n",
      "134\n",
      "133\n",
      "132\n",
      "131\n",
      "130\n",
      "129\n",
      "128\n",
      "127\n",
      "126\n",
      "125\n",
      "124\n",
      "123\n",
      "122\n",
      "121\n",
      "120\n",
      "119\n",
      "118\n",
      "117\n",
      "116\n",
      "115\n",
      "114\n",
      "113\n",
      "112\n",
      "111\n",
      "110\n",
      "109\n",
      "108\n",
      "107\n",
      "106\n",
      "105\n",
      "104\n",
      "103\n",
      "102\n",
      "101\n",
      "100\n",
      "99\n",
      "98\n",
      "97\n",
      "96\n",
      "95\n",
      "94\n",
      "93\n",
      "92\n",
      "91\n",
      "90\n",
      "89\n",
      "88\n",
      "87\n",
      "86\n",
      "85\n",
      "84\n",
      "83\n",
      "82\n",
      "81\n",
      "80\n",
      "79\n",
      "78\n",
      "77\n",
      "76\n",
      "75\n",
      "74\n",
      "73\n",
      "72\n",
      "71\n",
      "70\n",
      "69\n",
      "68\n",
      "67\n",
      "66\n",
      "65\n",
      "64\n",
      "63\n",
      "62\n",
      "61\n",
      "60\n",
      "59\n",
      "58\n",
      "57\n",
      "56\n",
      "55\n",
      "54\n",
      "53\n",
      "52\n",
      "51\n",
      "50\n",
      "49\n",
      "48\n",
      "47\n",
      "46\n",
      "45\n",
      "44\n",
      "43\n",
      "42\n",
      "41\n",
      "40\n",
      "39\n",
      "38\n",
      "37\n",
      "36\n",
      "35\n",
      "34\n",
      "33\n",
      "32\n",
      "31\n",
      "30\n",
      "29\n",
      "28\n",
      "27\n",
      "26\n",
      "25\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "✅ Generated 64 images in: ./SampledImgs/batch_1\n"
     ]
    }
   ],
   "source": [
    "generate_one_batch(\n",
    "    model_path=\"./Checkpoints/final_model.pt\",\n",
    "    save_dir=\"./SampledImgs/batch_1\",\n",
    "    batch_size=64,\n",
    "    image_size=128,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05e6dc06-2ea2-4365-97d0-4086665c7a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.4\n",
      "    Not uninstalling numpy at /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages, outside environment /apps/project/ondemand/app_jupyter/4.1.5\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/users/PFS0270/nikhilchatta/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Not uninstalling scipy at /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages, outside environment /apps/project/ondemand/app_jupyter/4.1.5\n",
      "    Can't uninstall 'scipy'. No files were found to uninstall.\n",
      "Successfully installed numpy-1.21.4 scipy-1.10.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade --force-reinstall numpy scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae292b5f-c77f-412c-9c37-21283407dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_10.png  sample_22.png  sample_34.png  sample_46.png  sample_58.png\n",
      "sample_11.png  sample_23.png  sample_35.png  sample_47.png  sample_59.png\n",
      "sample_12.png  sample_24.png  sample_36.png  sample_48.png  sample_5.png\n",
      "sample_13.png  sample_25.png  sample_37.png  sample_49.png  sample_60.png\n",
      "sample_14.png  sample_26.png  sample_38.png  sample_4.png   sample_61.png\n",
      "sample_15.png  sample_27.png  sample_39.png  sample_50.png  sample_62.png\n",
      "sample_16.png  sample_28.png  sample_3.png   sample_51.png  sample_63.png\n",
      "sample_17.png  sample_29.png  sample_40.png  sample_52.png  sample_64.png\n",
      "sample_18.png  sample_2.png   sample_41.png  sample_53.png  sample_6.png\n",
      "sample_19.png  sample_30.png  sample_42.png  sample_54.png  sample_7.png\n",
      "sample_1.png   sample_31.png  sample_43.png  sample_55.png  sample_8.png\n",
      "sample_20.png  sample_32.png  sample_44.png  sample_56.png  sample_9.png\n",
      "sample_21.png  sample_33.png  sample_45.png  sample_57.png\n"
     ]
    }
   ],
   "source": [
    "ls ./SampledImgs/batch_1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87676ea-1e11-4396-ba3c-2c50b80a8d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCheckpoints\u001b[0m/                               \u001b[01;34mDiffusionFreeGuidence\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/                                      MainCondition.py        README.md\n",
      "\u001b[01;34mDenoisingDiffusionProbabilityModel-ddpm-\u001b[0m/  Main.py                 \u001b[01;34mSampledImgs\u001b[0m/\n",
      "\u001b[01;34mDiffusion\u001b[0m/                                 Metrics.ipynb           Scheduler.py\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5036b49-29c3-4fd6-854b-c80621993e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/ddpm-div2k\n"
     ]
    }
   ],
   "source": [
    "cd ddpm-div2k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1d4f379-6784-422d-8af6-8418ae1b9f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 4.863290530561363\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_psnr(img1, img2, max_pixel=1.0):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "\n",
    "# Load generated image\n",
    "gen_img = Image.open(\"./SampledImgs/batch_1/sample_1.png\").convert(\"RGB\")\n",
    "gen_img = np.array(gen_img).astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "# Load ground truth image (adjust path if needed)\n",
    "gt_img = Image.open(\"./data/DIV2K_train_HR/0001.png\").convert(\"RGB\")\n",
    "gt_img = gt_img.resize((128, 128))  # Resize to match generated image\n",
    "gt_img = np.array(gt_img).astype(np.float32) / 255.0\n",
    "\n",
    "# Compute PSNR\n",
    "psnr_value = calculate_psnr(gen_img, gt_img)\n",
    "print(\"PSNR:\", psnr_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed538b6-fc01-4c55-8a72-745716fa1c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Round 1: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 2: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 3: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 4: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 5: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 6: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 7: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 8: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 9: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 10: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 11: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 12: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 13: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 14: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 15: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 16: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 17: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 18: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 19: Mean PSNR = 5.0102 dB\n",
      "🔁 Round 20: Mean PSNR = 5.0102 dB\n",
      "\n",
      "✅ Final Average PSNR over 20 rounds: 5.0102 dB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_psnr(img1, img2, max_pixel=1.0):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "\n",
    "def load_image(path, resize=(128, 128)):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(resize)\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "generated_dir = \"./SampledImgs/batch_1/\"\n",
    "groundtruth_dir = \"./data/DIV2K_train_HR/\"\n",
    "\n",
    "all_psnr_rounds = []\n",
    "\n",
    "for round_num in range(1, 21):\n",
    "    psnr_scores = []\n",
    "    for i in range(1, 65):\n",
    "        gen_path = os.path.join(generated_dir, f\"sample_{i}.png\")\n",
    "        gt_path = os.path.join(groundtruth_dir, f\"{str(i).zfill(4)}.png\")\n",
    "\n",
    "        if not os.path.exists(gen_path) or not os.path.exists(gt_path):\n",
    "            continue\n",
    "\n",
    "        gen_img = load_image(gen_path)\n",
    "        gt_img = load_image(gt_path)\n",
    "\n",
    "        psnr_val = calculate_psnr(gen_img, gt_img)\n",
    "        psnr_scores.append(psnr_val)\n",
    "\n",
    "    round_avg = np.mean(psnr_scores)\n",
    "    all_psnr_rounds.append(round_avg)\n",
    "    print(f\"🔁 Round {round_num}: Mean PSNR = {round_avg:.4f} dB\")\n",
    "\n",
    "final_avg = np.mean(all_psnr_rounds)\n",
    "print(\"\\n✅ Final Average PSNR over 20 rounds:\", round(final_avg, 4), \"dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e3acf26-4042-4705-abce-4b6c31040b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch_msssim\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from pytorch_msssim) (2.5.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.9/site-packages (from torch->pytorch_msssim) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch->pytorch_msssim) (4.12.2)\n",
      "Requirement already satisfied: networkx in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch->pytorch_msssim) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch->pytorch_msssim) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch->pytorch_msssim) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from jinja2->torch->pytorch_msssim) (3.0.2)\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: pytorch_msssim\n",
      "Successfully installed pytorch_msssim-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch_msssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d74aa32-7f30-4398-b627-59675160de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Round 1: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 2: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 3: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 4: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 5: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 6: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 7: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 8: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 9: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 10: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 11: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 12: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 13: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 14: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 15: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 16: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 17: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 18: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 19: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "🔁 Round 20: SSIM = 0.0029, MS-SSIM = 0.0120\n",
      "\n",
      "✅ Final SSIM over 20 rounds: 0.0029\n",
      "✅ Final MS-SSIM over 20 rounds: 0.012\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "\n",
    "def load_image_tensor(path, size=(128, 128)):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(size)\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "generated_dir = \"./SampledImgs/batch_1/\"\n",
    "groundtruth_dir = \"./data/DIV2K_train_HR/\"\n",
    "\n",
    "custom_weights = [0.4, 0.3, 0.3]  # Only 3 levels\n",
    "win_size = 7  # ✅ Keep this smaller to avoid assertion error\n",
    "\n",
    "all_ssim_rounds = []\n",
    "all_ms_ssim_rounds = []\n",
    "\n",
    "for round_num in range(1, 21):\n",
    "    ssim_scores = []\n",
    "    ms_ssim_scores = []\n",
    "\n",
    "    for i in range(1, 65):\n",
    "        gen_path = os.path.join(generated_dir, f\"sample_{i}.png\")\n",
    "        gt_path = os.path.join(groundtruth_dir, f\"{str(i).zfill(4)}.png\")\n",
    "\n",
    "        if not os.path.exists(gen_path) or not os.path.exists(gt_path):\n",
    "            continue\n",
    "\n",
    "        gen_tensor = load_image_tensor(gen_path)\n",
    "        gt_tensor = load_image_tensor(gt_path)\n",
    "\n",
    "        ssim_val = ssim(gen_tensor, gt_tensor, data_range=1.0).item()\n",
    "        ms_ssim_val = ms_ssim(\n",
    "            gen_tensor, gt_tensor, data_range=1.0,\n",
    "            weights=custom_weights,\n",
    "            win_size=win_size  # ✅ Reduced window\n",
    "        ).item()\n",
    "\n",
    "        ssim_scores.append(ssim_val)\n",
    "        ms_ssim_scores.append(ms_ssim_val)\n",
    "\n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "    avg_ms_ssim = np.mean(ms_ssim_scores)\n",
    "\n",
    "    all_ssim_rounds.append(avg_ssim)\n",
    "    all_ms_ssim_rounds.append(avg_ms_ssim)\n",
    "\n",
    "    print(f\"🔁 Round {round_num}: SSIM = {avg_ssim:.4f}, MS-SSIM = {avg_ms_ssim:.4f}\")\n",
    "\n",
    "final_ssim = np.mean(all_ssim_rounds)\n",
    "final_ms_ssim = np.mean(all_ms_ssim_rounds)\n",
    "\n",
    "print(\"\\n✅ Final SSIM over 20 rounds:\", round(final_ssim, 4))\n",
    "print(\"✅ Final MS-SSIM over 20 rounds:\", round(final_ms_ssim, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf8d8bc9-2e14-4176-90c0-1fda4acfec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error comparing image 0: [Errno 2] No such file or directory: './SampledImgs/batch_1/sample_0.png'\n",
      "\n",
      "✅ Final Average Metrics over 20 samples:\n",
      "Perceptual Similarity (L2): 184.1090\n",
      "Cosine Similarity: 0.1579\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# VGG Feature Extractor\n",
    "class VGGFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, layers=['relu2_2'], device='cpu'):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "        self.layers = layers\n",
    "        self.device = device\n",
    "        self.layer_mapping = {\n",
    "            'relu1_1': 0, 'relu1_2': 2,\n",
    "            'relu2_1': 5, 'relu2_2': 7,\n",
    "            'relu3_1': 10, 'relu3_2': 12, 'relu3_3': 14,\n",
    "            'relu4_1': 17, 'relu4_2': 19, 'relu4_3': 21,\n",
    "            'relu5_1': 24, 'relu5_2': 26, 'relu5_3': 28,\n",
    "        }\n",
    "        max_layer = max([self.layer_mapping[l] for l in layers])\n",
    "        self.model = vgg[:max_layer + 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        for name, layer in self.model._modules.items():\n",
    "            x = layer(x)\n",
    "            for key, idx in self.layer_mapping.items():\n",
    "                if int(name) == idx and key in self.layers:\n",
    "                    features[key] = x\n",
    "        return features\n",
    "\n",
    "# Image loader\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image_tensor(path, device='cpu'):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img = preprocess(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "# Loop for perceptual similarity\n",
    "device = 'cpu'\n",
    "model = VGGFeatureExtractor(layers=['relu2_2'], device=device)\n",
    "generated_dir = \"./SampledImgs/batch_1/\"\n",
    "gt_dir = \"./data/DIV2K_train_HR/\"\n",
    "image_indices = range(20)\n",
    "\n",
    "l2_scores, cosine_scores = [], []\n",
    "\n",
    "for i in image_indices:\n",
    "    try:\n",
    "        gen_path = os.path.join(generated_dir, f\"sample_{i}.png\")\n",
    "        gt_path = os.path.join(gt_dir, sorted(os.listdir(gt_dir))[i])\n",
    "\n",
    "        img1 = load_image_tensor(gen_path, device)\n",
    "        img2 = load_image_tensor(gt_path, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f1 = model(img1)['relu2_2']\n",
    "            f2 = model(img2)['relu2_2']\n",
    "\n",
    "        l2 = F.mse_loss(f1, f2).item()\n",
    "        cos_sim = F.cosine_similarity(f1.flatten(1), f2.flatten(1)).mean().item()\n",
    "\n",
    "        l2_scores.append(l2)\n",
    "        cosine_scores.append(cos_sim)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error comparing image {i}: {e}\")\n",
    "\n",
    "print(\"\\n✅ Final Average Metrics over 20 samples:\")\n",
    "print(f\"Perceptual Similarity (L2): {np.mean(l2_scores):.4f}\")\n",
    "print(f\"Cosine Similarity: {np.mean(cosine_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aac38b9-c614-4366-ada2-b36a76944c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: FID = 413.9255\n",
      "Round 2: FID = 413.5248\n",
      "Round 3: FID = 408.7175\n",
      "Round 4: FID = 427.1364\n",
      "Round 5: FID = 401.9757\n",
      "Round 6: FID = 450.8836\n",
      "Round 7: FID = 413.1036\n",
      "Round 8: FID = 423.8680\n",
      "Round 9: FID = 398.9120\n",
      "Round 10: FID = 435.7685\n",
      "Round 11: FID = 424.0186\n",
      "Round 12: FID = 403.8796\n",
      "Round 13: FID = 412.1881\n",
      "Round 14: FID = 391.4142\n",
      "Round 15: FID = 394.5985\n",
      "Round 16: FID = 433.2858\n",
      "Round 17: FID = 410.2757\n",
      "Round 18: FID = 441.5389\n",
      "Round 19: FID = 387.9187\n",
      "Round 20: FID = 408.0925\n",
      "\n",
      "✅ Average FID over 20 rounds = 414.7513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "414.75131604970267"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision import transforms\n",
    "from scipy import linalg\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Preprocess for InceptionV3\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Load InceptionV3 model\n",
    "inception = inception_v3(weights=Inception_V3_Weights.DEFAULT, transform_input=False)\n",
    "inception.fc = torch.nn.Identity()\n",
    "inception.eval().to(device)\n",
    "\n",
    "def get_activations(image_paths, model, batch_size=32):\n",
    "    activations = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch = []\n",
    "        for path in batch_paths:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img = preprocess(img).unsqueeze(0)  # [1,3,299,299]\n",
    "            batch.append(img)\n",
    "        batch = torch.cat(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            act = model(batch)  # [B,2048]\n",
    "        activations.append(act.cpu().numpy())\n",
    "    return np.concatenate(activations, axis=0)\n",
    "\n",
    "def calculate_fid(act1, act2):\n",
    "    mu1, sigma1 = np.mean(act1, axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(act2, axis=0), np.cov(act2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean): covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "    return fid\n",
    "\n",
    "# ==== Compute Average FID over 20 rounds ====\n",
    "def compute_fid_avg(gen_dir, gt_dir, rounds=20):\n",
    "    all_fid_scores = []\n",
    "    gen_images = sorted([os.path.join(gen_dir, f) for f in os.listdir(gen_dir) if f.endswith('.png')])\n",
    "    gt_images  = sorted([os.path.join(gt_dir, f) for f in os.listdir(gt_dir) if f.endswith('.png')])\n",
    "    \n",
    "    for i in range(rounds):\n",
    "        sampled_gen = np.random.choice(gen_images, 50, replace=False)\n",
    "        sampled_gt  = np.random.choice(gt_images, 50, replace=False)\n",
    "\n",
    "        act_gen = get_activations(sampled_gen, inception)\n",
    "        act_gt  = get_activations(sampled_gt, inception)\n",
    "\n",
    "        fid = calculate_fid(act_gen, act_gt)\n",
    "        all_fid_scores.append(fid)\n",
    "        print(f\"Round {i+1}: FID = {fid:.4f}\")\n",
    "\n",
    "    avg_fid = np.mean(all_fid_scores)\n",
    "    print(f\"\\n✅ Average FID over {rounds} rounds = {avg_fid:.4f}\")\n",
    "    return avg_fid\n",
    "\n",
    "# Example usage\n",
    "compute_fid_avg(\"./SampledImgs/batch_1\", \"./data/DIV2K_train_HR\", rounds=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30407633-5e49-4a68-bc6d-09d942984d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Inception Score: 1.0077 ± 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0076773, 0.002106546)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Preprocess for InceptionV3\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Load InceptionV3 (output logits before softmax)\n",
    "inception = inception_v3(weights=Inception_V3_Weights.DEFAULT, transform_input=False)\n",
    "inception.fc = torch.nn.Identity()\n",
    "inception.eval().to(device)\n",
    "\n",
    "def get_predictions(image_paths, model, batch_size=32):\n",
    "    preds = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch = []\n",
    "        for path in batch_paths:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img = preprocess(img).unsqueeze(0)  # [1, 3, 299, 299]\n",
    "            batch.append(img)\n",
    "        batch = torch.cat(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            softmax_preds = F.softmax(logits, dim=1)\n",
    "        preds.append(softmax_preds.cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "def calculate_inception_score(preds, splits=10):\n",
    "    N = preds.shape[0]\n",
    "    scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k + 1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        kl_div = part * (np.log(part + 1e-10) - np.log(py + 1e-10))\n",
    "        score = np.exp(np.mean(np.sum(kl_div, axis=1)))\n",
    "        scores.append(score)\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# ==== Run IS Evaluation ====\n",
    "def compute_inception_score(image_folder, splits=10):\n",
    "    image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.png')])\n",
    "    preds = get_predictions(image_paths, inception)\n",
    "    mean_is, std_is = calculate_inception_score(preds, splits=splits)\n",
    "    print(f\"\\n✅ Inception Score: {mean_is:.4f} ± {std_is:.4f}\")\n",
    "    return mean_is, std_is\n",
    "\n",
    "# Example usage\n",
    "compute_inception_score(\"./SampledImgs/batch_1\", splits=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7df22c5-d75f-44cb-b12a-bde0b64d8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 01 → Inception Score: 1.0077 ± 0.0021\n",
      " Folder ./SampledImgs/batch_2 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_3 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_4 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_5 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_6 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_7 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_8 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_9 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_10 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_11 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_12 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_13 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_14 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_15 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_16 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_17 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_18 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_19 does not exist. Skipping...\n",
      " Folder ./SampledImgs/batch_20 does not exist. Skipping...\n",
      "\n",
      " Final Avg Inception Score (over 1 rounds): 1.0077 ± 0.0021\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Load InceptionV3\n",
    "inception = inception_v3(weights=Inception_V3_Weights.DEFAULT, transform_input=False)\n",
    "inception.fc = torch.nn.Identity()\n",
    "inception.eval().to(device)\n",
    "\n",
    "def get_predictions(image_paths, model, batch_size=32):\n",
    "    preds = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch = []\n",
    "        for path in batch_paths:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img = preprocess(img).unsqueeze(0)\n",
    "            batch.append(img)\n",
    "        batch = torch.cat(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            softmax_preds = F.softmax(logits, dim=1)\n",
    "        preds.append(softmax_preds.cpu().numpy())\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "def calculate_inception_score(preds, splits=10):\n",
    "    N = preds.shape[0]\n",
    "    scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k + 1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        kl_div = part * (np.log(part + 1e-10) - np.log(py + 1e-10))\n",
    "        score = np.exp(np.mean(np.sum(kl_div, axis=1)))\n",
    "        scores.append(score)\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# ===== Safe Evaluation Over 20 Rounds =====\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "for round_num in range(1, 21):\n",
    "    image_folder = f\"./SampledImgs/batch_{round_num}\"\n",
    "    \n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\" Folder {image_folder} does not exist. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.png')])\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(f\" No images in {image_folder}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    preds = get_predictions(image_paths, inception)\n",
    "    mean_is, std_is = calculate_inception_score(preds, splits=10)\n",
    "    print(f\"Round {round_num:02d} → Inception Score: {mean_is:.4f} ± {std_is:.4f}\")\n",
    "    mean_list.append(mean_is)\n",
    "    std_list.append(std_is)\n",
    "\n",
    "# ==== Final Summary ====\n",
    "if mean_list:\n",
    "    overall_mean = np.mean(mean_list)\n",
    "    overall_std = np.mean(std_list)\n",
    "    print(f\"\\n Final Avg Inception Score (over {len(mean_list)} rounds): {overall_mean:.4f} ± {overall_std:.4f}\")\n",
    "else:\n",
    "    print(\" No valid rounds processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14b92f10-c3dd-47ff-b1eb-454e4cc22761",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input dimension should be at least 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m std_kids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m gt_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/DIV2K_train_HR\u001b[39m\u001b[38;5;124m\"\u001b[39m, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/DIV2K_train_HR\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)])[:\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m---> 69\u001b[0m gt_feats \u001b[38;5;241m=\u001b[39m \u001b[43mget_inception_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m round_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m):\n\u001b[1;32m     72\u001b[0m     image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./SampledImgs/batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mround_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn [34], line 36\u001b[0m, in \u001b[0;36mget_inception_features\u001b[0;34m(image_paths, model, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     35\u001b[0m         act \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[0;32m---> 36\u001b[0m         act \u001b[38;5;241m=\u001b[39m \u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m     feats\u001b[38;5;241m.\u001b[39mappend(act\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(feats, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:1381\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[0;32m-> 1381\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m \u001b[43m_list_with_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(\u001b[38;5;28minput\u001b[39m, _output_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/utils.py:41\u001b[0m, in \u001b[0;36m_list_with_default\u001b[0;34m(out_size, defaults)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_size\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(defaults) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_size):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput dimension should be at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     43\u001b[0m     v \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m v, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(out_size, defaults[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(out_size) :])\n\u001b[1;32m     44\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension should be at least 3"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Inception Model\n",
    "inception = inception_v3(weights=Inception_V3_Weights.DEFAULT, transform_input=False)\n",
    "inception.fc = torch.nn.Identity()\n",
    "inception.eval().to(device)\n",
    "\n",
    "def get_inception_features(image_paths, model, batch_size=32):\n",
    "    feats = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch = []\n",
    "        for path in batch_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = preprocess(img).unsqueeze(0)\n",
    "            batch.append(img)\n",
    "        batch = torch.cat(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            act = model(batch)\n",
    "            act = adaptive_avg_pool2d(act, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "        feats.append(act.cpu().numpy())\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "def polynomial_kernel(X, Y, degree=3, gamma=None, coef0=1):\n",
    "    if gamma is None:\n",
    "        gamma = 1.0 / X.shape[1]\n",
    "    K = (gamma * X.dot(Y.T) + coef0) ** degree\n",
    "    return K\n",
    "\n",
    "def calculate_kid(X_feats, Y_feats, subsets=10, subset_size=1000):\n",
    "    m = min(len(X_feats), len(Y_feats), subset_size)\n",
    "    kid_scores = []\n",
    "    for _ in range(subsets):\n",
    "        x_idx = np.random.choice(len(X_feats), m, replace=False)\n",
    "        y_idx = np.random.choice(len(Y_feats), m, replace=False)\n",
    "        X = X_feats[x_idx]\n",
    "        Y = Y_feats[y_idx]\n",
    "        K_XX = polynomial_kernel(X, X)\n",
    "        K_YY = polynomial_kernel(Y, Y)\n",
    "        K_XY = polynomial_kernel(X, Y)\n",
    "        m = float(m)\n",
    "        kid = (np.sum(K_XX) - np.trace(K_XX)) / (m * (m - 1)) + \\\n",
    "              (np.sum(K_YY) - np.trace(K_YY)) / (m * (m - 1)) - \\\n",
    "              2 * np.sum(K_XY) / (m * m)\n",
    "        kid_scores.append(kid)\n",
    "    return np.mean(kid_scores), np.std(kid_scores)\n",
    "\n",
    "# ===== KID Evaluation Over 20 Rounds =====\n",
    "mean_kids = []\n",
    "std_kids = []\n",
    "gt_images = sorted([os.path.join(\"data/DIV2K_train_HR\", f) for f in os.listdir(\"data/DIV2K_train_HR\") if f.endswith(\".png\")])[:100]\n",
    "\n",
    "gt_feats = get_inception_features(gt_images, inception)\n",
    "\n",
    "for round_num in range(1, 21):\n",
    "    image_folder = f\"./SampledImgs/batch_{round_num}\"\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(f\" {image_folder} missing. Skipping.\")\n",
    "        continue\n",
    "    gen_images = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(\".png\")])\n",
    "    if len(gen_images) == 0:\n",
    "        print(f\" No images in {image_folder}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    gen_feats = get_inception_features(gen_images, inception)\n",
    "    mean_kid, std_kid = calculate_kid(gen_feats, gt_feats, subsets=5)\n",
    "    print(f\"Round {round_num:02d} → KID: {mean_kid:.6f} ± {std_kid:.6f}\")\n",
    "    mean_kids.append(mean_kid)\n",
    "    std_kids.append(std_kid)\n",
    "\n",
    "# ==== Final KID Summary ====\n",
    "if mean_kids:\n",
    "    final_kid = np.mean(mean_kids)\n",
    "    final_kid_std = np.mean(std_kids)\n",
    "    print(f\"\\n Final Avg KID (20 rounds): {final_kid:.6f} ± {final_kid_std:.6f}\")\n",
    "else:\n",
    "    print(\" No valid rounds processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5941b94-8dee-49d0-a586-3bfecb806fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1: KID Mean = 0.1066, Std = 0.0008\n",
      " Folder not found: ./SampledImgs/batch_2. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_3. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_4. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_5. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_6. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_7. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_8. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_9. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_10. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_11. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_12. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_13. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_14. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_15. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_16. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_17. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_18. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_19. Skipping...\n",
      " Folder not found: ./SampledImgs/batch_20. Skipping...\n",
      "\n",
      " Final KID Scores over 20 rounds:\n",
      "Average KID Mean: 0.1066\n",
      "Average KID Std : 0.0008\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from scipy import linalg\n",
    "\n",
    "# Preprocessing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Inception Model (without classifier head)\n",
    "inception = models.inception_v3(pretrained=True, transform_input=False)\n",
    "inception.fc = nn.Identity()\n",
    "inception.to(device)\n",
    "inception.eval()\n",
    "\n",
    "# Function to get features\n",
    "def get_inception_features(image_paths, model, batch_size=32):\n",
    "    feats = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch = []\n",
    "        for path in batch_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = preprocess(img).unsqueeze(0)\n",
    "            batch.append(img)\n",
    "        batch = torch.cat(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            act = model(batch)  # already [B, 2048]\n",
    "        feats.append(act.cpu().numpy())\n",
    "    return np.concatenate(feats, axis=0)\n",
    "\n",
    "# Function to compute KID\n",
    "def polynomial_mmd_averages(codes_g, codes_r, num_subsets=50, subset_size=1000, degree=3, gamma=None, coef0=1):\n",
    "    m = subset_size\n",
    "    n_g = codes_g.shape[0]\n",
    "    n_r = codes_r.shape[0]\n",
    "    mmds = []\n",
    "\n",
    "    for _ in range(num_subsets):\n",
    "        g = codes_g[np.random.choice(n_g, m, replace=True)]\n",
    "        r = codes_r[np.random.choice(n_r, m, replace=True)]\n",
    "\n",
    "        k_rr = (r @ r.T / r.shape[1]) if gamma is None else (gamma * r @ r.T + coef0) ** degree\n",
    "        k_gg = (g @ g.T / g.shape[1]) if gamma is None else (gamma * g @ g.T + coef0) ** degree\n",
    "        k_rg = (r @ g.T / r.shape[1]) if gamma is None else (gamma * r @ g.T + coef0) ** degree\n",
    "\n",
    "        mmd = k_gg.mean() + k_rr.mean() - 2 * k_rg.mean()\n",
    "        mmds.append(mmd)\n",
    "\n",
    "    return np.mean(mmds), np.std(mmds)\n",
    "\n",
    "# === Main Evaluation ===\n",
    "mean_kids = []\n",
    "std_kids = []\n",
    "\n",
    "# Ground truth features (you can cache these if needed)\n",
    "gt_dir = \"data/DIV2K_train_HR\"\n",
    "gt_images = sorted([os.path.join(gt_dir, f) for f in os.listdir(gt_dir) if f.endswith(\".png\")])[:100]\n",
    "gt_feats = get_inception_features(gt_images, inception)\n",
    "\n",
    "for round_num in range(1, 21):\n",
    "    image_folder = f\"./SampledImgs/batch_{round_num}\"\n",
    "    if not os.path.isdir(image_folder):\n",
    "        print(f\" Folder not found: {image_folder}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    gen_images = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(\".png\")])\n",
    "    if len(gen_images) == 0:\n",
    "        print(f\" No images in {image_folder}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    gen_feats = get_inception_features(gen_images, inception)\n",
    "\n",
    "    mean_kid, std_kid = polynomial_mmd_averages(gen_feats, gt_feats)\n",
    "    mean_kids.append(mean_kid)\n",
    "    std_kids.append(std_kid)\n",
    "\n",
    "    print(f\" Round {round_num}: KID Mean = {mean_kid:.4f}, Std = {std_kid:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n Final KID Scores over 20 rounds:\")\n",
    "print(f\"Average KID Mean: {np.mean(mean_kids):.4f}\")\n",
    "print(f\"Average KID Std : {np.mean(std_kids):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "272165a2-2b43-4da3-b077-033bf5b39f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lpips in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from lpips) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from lpips) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from lpips) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from lpips) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from lpips) (4.67.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
      "Requirement already satisfied: networkx in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib64/python3.9/site-packages (from torchvision>=0.2.1->lpips) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /apps/project/ondemand/app_jupyter/4.1.5/lib/python3.9/site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bd28c68-c9fa-4b04-9f91-5f337661cc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /users/PFS0270/nikhilchatta/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:00<00:00, 479MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PFS0270/nikhilchatta/.local/lib/python3.9/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Round 1 - LPIPS: 0.8485\n",
      " Missing batch 2, skipping.\n",
      " Missing batch 3, skipping.\n",
      " Missing batch 4, skipping.\n",
      " Missing batch 5, skipping.\n",
      " Missing batch 6, skipping.\n",
      " Missing batch 7, skipping.\n",
      " Missing batch 8, skipping.\n",
      " Missing batch 9, skipping.\n",
      " Missing batch 10, skipping.\n",
      " Missing batch 11, skipping.\n",
      " Missing batch 12, skipping.\n",
      " Missing batch 13, skipping.\n",
      " Missing batch 14, skipping.\n",
      " Missing batch 15, skipping.\n",
      " Missing batch 16, skipping.\n",
      " Missing batch 17, skipping.\n",
      " Missing batch 18, skipping.\n",
      " Missing batch 19, skipping.\n",
      " Missing batch 20, skipping.\n",
      "\n",
      " Final LPIPS Summary over 20 rounds:\n",
      "Average LPIPS: 0.8485\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import lpips\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load LPIPS model (AlexNet or VGG)\n",
    "lpips_model = lpips.LPIPS(net='alex').to(device)  # Use 'vgg' or 'alex'\n",
    "\n",
    "# Preprocess images to [-1, 1] for LPIPS\n",
    "lpips_preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize for LPIPS model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Function to load and prepare image\n",
    "def load_lpips_image(path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = lpips_preprocess(img).unsqueeze(0).to(device)\n",
    "    return img\n",
    "\n",
    "# LPIPS evaluation\n",
    "lpips_scores = []\n",
    "\n",
    "gt_images = sorted([os.path.join(\"data/DIV2K_train_HR\", f) for f in os.listdir(\"data/DIV2K_train_HR\") if f.endswith(\".png\")])[:64]\n",
    "\n",
    "for round_num in range(1, 21):\n",
    "    batch_folder = f\"./SampledImgs/batch_{round_num}\"\n",
    "    if not os.path.isdir(batch_folder):\n",
    "        print(f\" Missing batch {round_num}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    gen_images = sorted([os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.endswith(\".png\")])\n",
    "    if len(gen_images) == 0:\n",
    "        print(f\" No images in batch {round_num}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    round_scores = []\n",
    "    for gen_path, gt_path in zip(gen_images, gt_images):\n",
    "        gen_tensor = load_lpips_image(gen_path)\n",
    "        gt_tensor  = load_lpips_image(gt_path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            d = lpips_model(gen_tensor, gt_tensor).item()\n",
    "        round_scores.append(d)\n",
    "\n",
    "    avg_lpips = sum(round_scores) / len(round_scores)\n",
    "    lpips_scores.append(avg_lpips)\n",
    "    print(f\" Round {round_num} - LPIPS: {avg_lpips:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n Final LPIPS Summary over 20 rounds:\")\n",
    "print(f\"Average LPIPS: {np.mean(lpips_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcf06a-d710-4997-bbfa-dbfc0a5c21f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
